{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4fc037-e6f0-44fa-bb0b-d64122b271a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:18:08.175187: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-30 01:18:08.187312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753831088.201375  128352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753831088.204656  128352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753831088.213183  128352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753831088.213201  128352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753831088.213202  128352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753831088.213203  128352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-30 01:18:08.217936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/tapiwa/anaconda3/envs/tfu/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Decision Transformer Training =====\n",
      "Collecting training data...\n",
      "Episode 0/1000\n",
      "Episode 100/1000\n",
      "Episode 200/1000\n",
      "Episode 300/1000\n",
      "Episode 400/1000\n",
      "Episode 500/1000\n",
      "Episode 600/1000\n",
      "Episode 700/1000\n",
      "Episode 800/1000\n",
      "Episode 900/1000\n",
      "Collected 1000 trajectories\n",
      "Training data shape - States: (1860, 20, 305), Actions: (1860, 20), RTG: (1860, 20), Targets: (1860, 20)\n",
      "Training Decision Transformer...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753831089.912755  128352 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1148 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753831095.682609  129490 service.cc:152] XLA service 0x77eb5000d4a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753831095.682624  129490 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-07-30 01:18:15.821255: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-30 01:18:16.217435: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1753831096.707779  129490 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-30 01:18:17.629397: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:18.226038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:18.556453: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:19.059247: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125_0', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:19.713974: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:19.741551: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 424 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:19.914543: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:20.026312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_42', 112 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:20.312065: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:20.346221: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 112 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:21.233648: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_42', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:22.132845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:22.705252: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 416 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.069744: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 460 bytes spill stores, 460 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.235251: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.242669: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.420585: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.645021: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:23.945868: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 480 bytes spill stores, 480 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:24.014879: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:24.299354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:24.378041: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:24.568693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:24.630254: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/47\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2844 - loss: 1.6664   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753831109.237667  129490 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3039 - loss: 1.5653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:18:30.027108: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-07-30 01:18:31.428814: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:31.658339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:31.986423: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:32.102261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:32.362594: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_42', 112 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:32.584299: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 112 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:32.805719: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:33.916669: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 172 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:34.203966: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:34.427449: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_42', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:35.121339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:35.668845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 416 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:35.923146: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:35.943847: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:36.898716: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.566008: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.572462: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.650934: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.744041: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.811166: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11281', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.930474: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 460 bytes spill stores, 460 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.946713: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 480 bytes spill stores, 480 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:37.972152: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11392', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.3066 - loss: 1.5551"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:18:43.355356: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-07-30 01:18:44.822448: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-07-30 01:18:45.507506: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:45.579927: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 104 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:46.301992: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:46.726466: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_40', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:46.727073: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:18:46.793365: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 403ms/step - accuracy: 0.3074 - loss: 1.5520 - val_accuracy: 0.3629 - val_loss: 1.3177\n",
      "Epoch 2/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4705 - loss: 1.1919 - val_accuracy: 0.4192 - val_loss: 1.2900\n",
      "Epoch 3/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5594 - loss: 1.0485 - val_accuracy: 0.4528 - val_loss: 1.2818\n",
      "Epoch 4/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6223 - loss: 0.9348 - val_accuracy: 0.4710 - val_loss: 1.2895\n",
      "Epoch 5/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6724 - loss: 0.8359 - val_accuracy: 0.4746 - val_loss: 1.3241\n",
      "Epoch 6/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7251 - loss: 0.7314 - val_accuracy: 0.4847 - val_loss: 1.3826\n",
      "Epoch 7/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7684 - loss: 0.6327 - val_accuracy: 0.4816 - val_loss: 1.4694\n",
      "Epoch 8/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8164 - loss: 0.5341 - val_accuracy: 0.4780 - val_loss: 1.5742\n",
      "Epoch 9/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8534 - loss: 0.4442 - val_accuracy: 0.4789 - val_loss: 1.6969\n",
      "Epoch 10/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8823 - loss: 0.3689 - val_accuracy: 0.4851 - val_loss: 1.8271\n",
      "Epoch 11/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9080 - loss: 0.3002 - val_accuracy: 0.4883 - val_loss: 2.0068\n",
      "Epoch 12/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9268 - loss: 0.2454 - val_accuracy: 0.4831 - val_loss: 2.1558\n",
      "Epoch 13/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9414 - loss: 0.2014 - val_accuracy: 0.4743 - val_loss: 2.3198\n",
      "Epoch 14/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9501 - loss: 0.1718 - val_accuracy: 0.4694 - val_loss: 2.5051\n",
      "Epoch 15/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9595 - loss: 0.1400 - val_accuracy: 0.4669 - val_loss: 2.6389\n",
      "Epoch 16/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9681 - loss: 0.1171 - val_accuracy: 0.4700 - val_loss: 2.7915\n",
      "Epoch 17/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9710 - loss: 0.1011 - val_accuracy: 0.4715 - val_loss: 2.9465\n",
      "Epoch 18/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9761 - loss: 0.0870 - val_accuracy: 0.4692 - val_loss: 3.0579\n",
      "Epoch 19/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.0788 - val_accuracy: 0.4616 - val_loss: 3.2090\n",
      "Epoch 20/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9794 - loss: 0.0693 - val_accuracy: 0.4642 - val_loss: 3.3289\n",
      "Epoch 21/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9837 - loss: 0.0576 - val_accuracy: 0.4652 - val_loss: 3.4088\n",
      "Epoch 22/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0519 - val_accuracy: 0.4652 - val_loss: 3.5508\n",
      "Epoch 23/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9885 - loss: 0.0432 - val_accuracy: 0.4634 - val_loss: 3.6610\n",
      "Epoch 24/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9879 - loss: 0.0411 - val_accuracy: 0.4633 - val_loss: 3.7906\n",
      "Epoch 25/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9896 - loss: 0.0383 - val_accuracy: 0.4636 - val_loss: 3.9274\n",
      "Epoch 26/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9921 - loss: 0.0302 - val_accuracy: 0.4609 - val_loss: 3.9748\n",
      "Epoch 27/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9923 - loss: 0.0284 - val_accuracy: 0.4620 - val_loss: 4.0803\n",
      "Epoch 28/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9912 - loss: 0.0271 - val_accuracy: 0.4648 - val_loss: 4.1471\n",
      "Epoch 29/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0244 - val_accuracy: 0.4667 - val_loss: 4.1931\n",
      "Epoch 30/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0220 - val_accuracy: 0.4530 - val_loss: 4.2858\n",
      "Epoch 31/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9927 - loss: 0.0235 - val_accuracy: 0.4597 - val_loss: 4.4113\n",
      "Epoch 32/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9945 - loss: 0.0195 - val_accuracy: 0.4605 - val_loss: 4.4282\n",
      "Epoch 33/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9956 - loss: 0.0157 - val_accuracy: 0.4657 - val_loss: 4.5316\n",
      "Epoch 34/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9950 - loss: 0.0173 - val_accuracy: 0.4606 - val_loss: 4.5436\n",
      "Epoch 35/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0159 - val_accuracy: 0.4651 - val_loss: 4.5558\n",
      "Epoch 36/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0131 - val_accuracy: 0.4629 - val_loss: 4.6685\n",
      "Epoch 37/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9953 - loss: 0.0149 - val_accuracy: 0.4633 - val_loss: 4.6548\n",
      "Epoch 38/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9964 - loss: 0.0129 - val_accuracy: 0.4630 - val_loss: 4.7440\n",
      "Epoch 39/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9967 - loss: 0.0102 - val_accuracy: 0.4636 - val_loss: 4.7469\n",
      "Epoch 40/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9971 - loss: 0.0097 - val_accuracy: 0.4559 - val_loss: 4.8495\n",
      "Epoch 41/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9969 - loss: 0.0097 - val_accuracy: 0.4569 - val_loss: 4.9575\n",
      "Epoch 42/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9963 - loss: 0.0101 - val_accuracy: 0.4569 - val_loss: 5.0208\n",
      "Epoch 43/50\n",
      "\u001b[1m36/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9968 - loss: 0.0118 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Set Random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class SnakeGame:\n",
    "    \"\"\"Snake game environment for Decision Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, width=12, height=12):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game to initial state\"\"\"\n",
    "        center_x, center_y = self.width // 2, self.height // 2\n",
    "        self.snake = [(center_x, center_y), (center_x - 1, center_y), (center_x - 2, center_y)]\n",
    "        self.direction = 1  # 0=up, 1=right, 2=down, 3=left\n",
    "        self.food = self._place_food()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        self.steps_without_food = 0\n",
    "        self.max_steps_without_food = 100\n",
    "        return self.get_state()\n",
    "\n",
    "    def _place_food(self):\n",
    "        \"\"\"Place food randomly, avoiding snake body\"\"\"\n",
    "        while True:\n",
    "            food = (random.randint(0, self.width - 1), random.randint(0, self.height - 1))\n",
    "            if food not in self.snake:\n",
    "                return food\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Get current game state as a feature vector\"\"\"\n",
    "        state = np.zeros((self.height, self.width, 3))  # 3 channels: snake, food, head\n",
    "\n",
    "        # Channel 0: Snake Body\n",
    "        for i, (x, y) in enumerate(self.snake):\n",
    "            if 0 <= x < self.width and 0 <= y < self.height:\n",
    "                state[y, x, 0] = 1.0\n",
    "\n",
    "        # Channel 1: Food\n",
    "        if 0 <= self.food[0] < self.width and 0 <= self.food[1] < self.height:\n",
    "            state[self.food[1], self.food[0], 1] = 1.0\n",
    "\n",
    "        # Channel 2: Snake head\n",
    "        head_x, head_y = self.snake[0]\n",
    "        if 0 <= head_x < self.width and 0 <= head_y < self.height:\n",
    "            state[head_y, head_x, 2] = 1.0\n",
    "\n",
    "        flat_state = state.flatten()\n",
    "\n",
    "        # Additional normalized features\n",
    "        food_x, food_y = self.food\n",
    "        additional_features = [\n",
    "            self.direction / 3.0,\n",
    "            (food_x - head_x) / self.width,\n",
    "            (food_y - head_y) / self.height,\n",
    "            len(self.snake) / (self.width * self.height),\n",
    "            self.steps_without_food / self.max_steps_without_food\n",
    "        ]\n",
    "        return np.concatenate([flat_state, additional_features])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one game step.\n",
    "        Actions: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "        Returns: next_state, reward, done\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0, True\n",
    "\n",
    "        # Prevent direct reversal\n",
    "        if action != (self.direction + 2) % 4:\n",
    "            self.direction = action\n",
    "\n",
    "        # Calculate new head position\n",
    "        head_x, head_y = self.snake[0]\n",
    "        if self.direction == 0:  # up\n",
    "            new_head = (head_x, head_y - 1)\n",
    "        elif self.direction == 1:  # right\n",
    "            new_head = (head_x + 1, head_y)\n",
    "        elif self.direction == 2:  # down\n",
    "            new_head = (head_x, head_y + 1)\n",
    "        else:  # left\n",
    "            new_head = (head_x - 1, head_y)\n",
    "\n",
    "        # Check wall collision\n",
    "        if (new_head[0] < 0 or new_head[0] >= self.width or\n",
    "                new_head[1] < 0 or new_head[1] >= self.height):\n",
    "            self.done = True\n",
    "            return self.get_state(), -10, self.done\n",
    "\n",
    "        # Check self collision\n",
    "        if new_head in self.snake:\n",
    "            self.done = True\n",
    "            return self.get_state(), -10, self.done\n",
    "\n",
    "        # Move snake\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        # Check if food is eaten\n",
    "        if new_head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self.food = self._place_food()\n",
    "            self.steps_without_food = 0\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "            reward = -0.1\n",
    "            self.steps_without_food += 1\n",
    "\n",
    "        # Starvation condition\n",
    "        if self.steps_without_food >= self.max_steps_without_food:\n",
    "            self.done = True\n",
    "            reward = -5\n",
    "\n",
    "        # Bonus for moving closer to food\n",
    "        food_x, food_y = self.food\n",
    "        distance_to_food = abs(head_x - food_x) + abs(head_y - food_y)\n",
    "        reward += 0.1 / (distance_to_food + 1)\n",
    "\n",
    "        return self.get_state(), reward, self.done\n",
    "\n",
    "    def render_console(self):\n",
    "        \"\"\"Print game state to console\"\"\"\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Place snake\n",
    "        for i, (x, y) in enumerate(self.snake):\n",
    "            if 0 <= x < self.width and 0 <= y < self.height:\n",
    "                grid[y][x] = 'H' if i == 0 else 'S'\n",
    "\n",
    "        # Place food\n",
    "        food_x, food_y = self.food\n",
    "        if 0 <= food_x < self.width and 0 <= food_y < self.height:\n",
    "            grid[food_y][food_x] = 'F'\n",
    "\n",
    "        # Print grid\n",
    "        print(f\"Score: {self.score}, Steps without food: {self.steps_without_food}\")\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n",
    "\n",
    "\n",
    "# ======================= Decision Transformer Model =======================\n",
    "class DecisionTransformer(keras.Model):\n",
    "    def __init__(self, state_dim, action_dim=4, max_length=50, embed_dim=256, num_heads=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_length = max_length\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Embedding layers\n",
    "        self.state_embed = keras.Sequential([\n",
    "            layers.Dense(embed_dim, activation='relu'),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "        self.action_embed = layers.Embedding(action_dim, embed_dim)\n",
    "\n",
    "        self.return_embed = keras.Sequential([\n",
    "            layers.Dense(embed_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "        self.pos_embed = layers.Embedding(3 * max_length, embed_dim)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = []\n",
    "        for _ in range(num_layers):\n",
    "            self.transformer_blocks.append({\n",
    "                'attention': layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads),\n",
    "                'dropout1': layers.Dropout(0.1),\n",
    "                'norm1': layers.LayerNormalization(),\n",
    "                'ff1': layers.Dense(embed_dim * 2, activation='gelu'),\n",
    "                'ff2': layers.Dense(embed_dim),\n",
    "                'dropout2': layers.Dropout(0.1),\n",
    "                'norm2': layers.LayerNormalization()\n",
    "            })\n",
    "\n",
    "        # Output head\n",
    "        self.action_head = keras.Sequential([\n",
    "            layers.Dense(embed_dim, activation='relu'),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Dense(action_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        states = inputs['states']\n",
    "        actions = inputs['actions']\n",
    "        return_to_go = inputs['return_to_go']\n",
    "        \n",
    "        batch_size = tf.shape(states)[0]\n",
    "        seq_len = tf.shape(states)[1]\n",
    "\n",
    "        # Embeddings\n",
    "        state_embeddings = self.state_embed(states)\n",
    "        # Convert actions to int32 for embedding\n",
    "        actions_int = tf.cast(actions, tf.int32)\n",
    "        action_embeddings = self.action_embed(actions_int)\n",
    "        return_embeddings = self.return_embed(tf.expand_dims(return_to_go, -1))\n",
    "\n",
    "        # Build sequence (interleaved: return, state, action)\n",
    "        sequence = tf.stack([return_embeddings, state_embeddings, action_embeddings], axis=3)\n",
    "        sequence = tf.reshape(sequence, (batch_size, seq_len * 3, self.embed_dim))\n",
    "\n",
    "        # Add positional embeddings\n",
    "        positions = tf.range(seq_len * 3)\n",
    "        pos_embeddings = self.pos_embed(positions)\n",
    "        sequence += pos_embeddings\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        x = sequence\n",
    "        for block in self.transformer_blocks:\n",
    "            # Multi-head attention\n",
    "            attn_output = block['attention'](x, x, training=training)\n",
    "            attn_output = block['dropout1'](attn_output, training=training)\n",
    "            x = block['norm1'](x + attn_output)\n",
    "\n",
    "            # Feed forward\n",
    "            ff_output = block['ff1'](x)\n",
    "            ff_output = block['ff2'](ff_output)\n",
    "            ff_output = block['dropout2'](ff_output, training=training)\n",
    "            x = block['norm2'](x + ff_output)\n",
    "\n",
    "        # Extract action tokens (every 3rd token starting from index 2)\n",
    "        action_tokens = x[:, 2::3, :]\n",
    "        action_logits = self.action_head(action_tokens)\n",
    "        return action_logits\n",
    "\n",
    "\n",
    "# ======================= Data Collection =======================\n",
    "def collect_random_data(env, num_episodes=500, max_steps=200):\n",
    "    trajectories = []\n",
    "    print(\"Collecting training data...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes}\")\n",
    "\n",
    "        env.reset()\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            state = env.get_state()\n",
    "\n",
    "            head_x, head_y = env.snake[0]\n",
    "            food_x, food_y = env.food\n",
    "\n",
    "            # Smart action selection 70% of the time\n",
    "            if random.random() < 0.7:\n",
    "                dx = food_x - head_x\n",
    "                dy = food_y - head_y\n",
    "\n",
    "                if abs(dx) > abs(dy):\n",
    "                    action = 1 if dx > 0 else 3  # right or left\n",
    "                else:\n",
    "                    action = 2 if dy > 0 else 0  # down or up\n",
    "\n",
    "                # Avoid walls\n",
    "                if action == 0 and head_y == 0:\n",
    "                    action = random.choice([1, 3])\n",
    "                elif action == 1 and head_x == env.width - 1:\n",
    "                    action = random.choice([0, 2])\n",
    "                elif action == 2 and head_y == env.height - 1:\n",
    "                    action = random.choice([1, 3])\n",
    "                elif action == 3 and head_x == 0:\n",
    "                    action = random.choice([0, 2])\n",
    "            else:\n",
    "                action = random.randint(0, 3)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate returns-to-go\n",
    "        returns = []\n",
    "        rtg = 0\n",
    "        for r in reversed(rewards):\n",
    "            rtg += r\n",
    "            returns.insert(0, rtg)\n",
    "\n",
    "        if len(states) > 1:\n",
    "            trajectories.append({\n",
    "                'states': np.array(states),\n",
    "                'actions': np.array(actions),\n",
    "                'rewards': np.array(rewards),\n",
    "                'returns': np.array(returns)\n",
    "            })\n",
    "\n",
    "    print(f\"Collected {len(trajectories)} trajectories\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "# ======================= Preprocess Training Data =======================\n",
    "def create_training_data(trajectories, max_length=20):\n",
    "    states_batch, actions_batch, rtg_batch, targets_batch = [], [], [], []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        states = traj['states']\n",
    "        actions = traj['actions']\n",
    "        returns = traj['returns']\n",
    "\n",
    "        for i in range(len(states) - 1):\n",
    "            seq_len = min(max_length, len(states) - i)\n",
    "\n",
    "            s_seq = states[i:i+seq_len]\n",
    "            a_seq = actions[i:i+seq_len]\n",
    "            r_seq = returns[i:i+seq_len]\n",
    "\n",
    "            # Pad sequences to max_length\n",
    "            pad_len = max_length - seq_len\n",
    "            if pad_len > 0:\n",
    "                s_seq = np.concatenate([np.zeros((pad_len, s_seq.shape[1])), s_seq])\n",
    "                a_seq = np.concatenate([np.zeros(pad_len), a_seq])\n",
    "                r_seq = np.concatenate([np.zeros(pad_len), r_seq])\n",
    "\n",
    "            # Create targets (next actions)\n",
    "            if i + seq_len < len(actions):\n",
    "                target = actions[i+1:i+seq_len+1]\n",
    "                if len(target) < max_length:\n",
    "                    target = np.concatenate([np.zeros(max_length - len(target)), target])\n",
    "\n",
    "                states_batch.append(s_seq)\n",
    "                actions_batch.append(a_seq)\n",
    "                rtg_batch.append(r_seq)\n",
    "                targets_batch.append(target)\n",
    "\n",
    "    return (np.array(states_batch), np.array(actions_batch),\n",
    "            np.array(rtg_batch), np.array(targets_batch))\n",
    "\n",
    "\n",
    "# ======================= Train the Model =======================\n",
    "def train_decision_transformer():\n",
    "    env = SnakeGame(width=10, height=10)\n",
    "    trajectories = collect_random_data(env, num_episodes=1000, max_steps=150)\n",
    "    states, actions, returns_to_go, targets = create_training_data(trajectories)\n",
    "\n",
    "    print(f\"Training data shape - States: {states.shape}, Actions: {actions.shape}, RTG: {returns_to_go.shape}, Targets: {targets.shape}\")\n",
    "\n",
    "    model = DecisionTransformer(\n",
    "        state_dim=states.shape[-1],\n",
    "        action_dim=4,\n",
    "        max_length=20,\n",
    "        embed_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=4\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"Training Decision Transformer...\")\n",
    "    history = model.fit(\n",
    "        x={\"states\": states, \"actions\": actions, \"return_to_go\": returns_to_go},\n",
    "        y=targets,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, env, history\n",
    "\n",
    "\n",
    "# ======================= Evaluate Model =======================\n",
    "def play_with_dt(model, env, target_return=15, render=True):\n",
    "    env.reset()\n",
    "    states = [env.get_state()]\n",
    "    actions = [0]\n",
    "    returns_to_go = [target_return]\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not env.done and steps < 200:\n",
    "        seq_len = min(20, len(states))\n",
    "        s_seq = np.array(states[-seq_len:])\n",
    "        a_seq = np.array(actions[-seq_len:])\n",
    "        r_seq = np.array(returns_to_go[-seq_len:])\n",
    "\n",
    "        # Pad sequences\n",
    "        pad_len = 20 - seq_len\n",
    "        if pad_len > 0:\n",
    "            s_seq = np.concatenate([np.zeros((pad_len, s_seq.shape[1])), s_seq])\n",
    "            a_seq = np.concatenate([np.zeros(pad_len), a_seq])\n",
    "            r_seq = np.concatenate([np.zeros(pad_len), r_seq])\n",
    "\n",
    "        # Create batch\n",
    "        s_batch = s_seq[np.newaxis, ...]\n",
    "        a_batch = a_seq[np.newaxis, ...]\n",
    "        r_batch = r_seq[np.newaxis, ...]\n",
    "\n",
    "        # Get action prediction\n",
    "        action_logits = model({\"states\": s_batch, \"actions\": a_batch, \"return_to_go\": r_batch}, training=False)\n",
    "        action_probs = tf.nn.softmax(action_logits[0, -1])\n",
    "        action = tf.argmax(action_probs).numpy()\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Update sequences\n",
    "        states.append(next_state)\n",
    "        actions.append(action)\n",
    "        returns_to_go.append(returns_to_go[-1] - reward)\n",
    "\n",
    "        if render and steps % 5 == 0:\n",
    "            print(f\"Step {steps}, Score: {env.score}, Total Reward: {total_reward:.2f}\")\n",
    "            env.render_console()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    print(f\"Game Over! Final Score: {env.score}, Total Reward: {total_reward:.2f}\")\n",
    "    return env.score, total_reward\n",
    "\n",
    "\n",
    "# ======================= Main =======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"===== Decision Transformer Training =====\")\n",
    "    model, env, history = train_decision_transformer()\n",
    "\n",
    "    print(\"\\n=== Testing Trained Model ===\")\n",
    "    for target_return in [10, 20, 30]:\n",
    "        print(f\"\\nTarget return: {target_return}\")\n",
    "        score, reward = play_with_dt(model, env, target_return=target_return, render=True)\n",
    "        print(f\"Score: {score}, Reward: {reward:.2f}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n=== Demo Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f2164-e9a7-49d2-aaa6-1afe2a1a023d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfu",
   "language": "python",
   "name": "tfu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
